# -*- coding: utf-8 -*-
"""Bidirectional LSTM COVID Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V2SgqymcTnUrU-enV1E_rjMwXvDwJW6_
"""

import numpy as np

import sys

import pandas as pd

"""**IMPORTING DATASET**"""


train_df = pd.read_csv(r"C:\Users\KARTHIKA\Documents\Final project\Corona_NLP_train.csv", encoding='latin1')


test_df = pd.read_csv(r"C:\Users\KARTHIKA\Documents\Final project\Corona_NLP_test.csv", encoding='latin1')

train_df.shape,test_df.shape

train_df.head()

test_df.head()

"""**NULL VALUES**"""

train_df.isnull().sum()

test_df.isnull().sum()

def convert_Sentiment(Sentiment):
    if Sentiment == "Extremely Positive":
        return 2
    elif Sentiment == "Extremely Negative":
        return 0
    elif Sentiment == "Positive":
        return 2
    elif Sentiment == "Negative":
        return 0
    else:
        return 1

train_df.Sentiment = train_df.Sentiment.apply(lambda x : convert_Sentiment(x))

test_df.Sentiment = test_df.Sentiment.apply(lambda x : convert_Sentiment(x))

"""**TEXT PREPROCESSING**"""

"""**Converting back to String**"""

train_df['text'] = train_df.OriginalTweet
train_df["text"] = train_df["text"].astype(str)

test_df['text'] = test_df.OriginalTweet
test_df["text"] = test_df["text"].astype(str)

"""**Removing URL**"""

import re

def remove_urls(text):
    url_remove = re.compile(r'https?://\S+|www\.\S+')
    return url_remove.sub(r'', text)
train_df['no_url']=train_df['text'].apply(lambda x:remove_urls(x))
test_df['no_url']=test_df['text'].apply(lambda x:remove_urls(x))

"""**Removing mention**

"""

def remove_mention(x):
    text=re.sub(r'@\w+','',x)
    return text
train_df['no_mention']=train_df['no_url'].apply(lambda x:remove_mention(x))
test_df['no_mention']=test_df['no_url'].apply(lambda x:remove_mention(x))

"""**Removing HTML**"""

def remove_html(text):
    html=re.compile(r'<.*?>')
    return html.sub(r'',text)
train_df['no_html']=train_df['no_mention'].apply(lambda x:remove_html(x))
test_df['no_html']=test_df['no_mention'].apply(lambda x:remove_html(x))

"""**Removing Hash**"""

def remove_hash(x):
    text=re.sub(r'#\w+','',x)
    return text
train_df['no_hash']=train_df['no_html'].apply(lambda x:remove_hash(x))
test_df['no_hash']=test_df['no_html'].apply(lambda x:remove_hash(x))

"""**Removing number**"""

def remove_num(text):
    remove= re.sub(r'\d+', '', text)
    return remove
train_df['no_num']=train_df['no_hash'].apply(lambda x:remove_num(x))
test_df['no_num']=test_df['no_hash'].apply(lambda x:remove_num(x))

"""**Tokenisation**"""

import nltk
# import SyllableTokenizer() method from nltk
from nltk import word_tokenize

train_df['tokenized'] = train_df['no_num'].apply(word_tokenize)
train_df.head()

test_df['tokenized'] = test_df['no_num'].apply(word_tokenize)
test_df.head()

"""***Converting all Characters to Lowercase***

"""

train_df['lower'] = train_df['tokenized'].apply(lambda x: [word.lower() for word in x])
train_df.head()

test_df['lower'] = test_df['tokenized'].apply(lambda x: [word.lower() for word in x])
test_df.head()

"""**Removing Punctuations**"""

punc = '''!()-[]{};:'"\,<>./?$%^&*_~'''
train_df['no_punc'] = train_df['lower'].apply(lambda x: [word for word in x if word not in punc])
train_df.head()

test_df['no_punc'] = test_df['lower'].apply(lambda x: [word for word in x if word not in punc])
test_df.head()

"""**Removing Stopwords**"""

from nltk.corpus import stopwords


stop_words = set(stopwords.words('english'))
train_df['stopwords_removed'] = train_df['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])
train_df.head()

stop_words = set(stopwords.words('english'))
test_df['stopwords_removed'] = test_df['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])
test_df.head()

"""**Lemmatization**"""


train_df['pos_tags'] = train_df['stopwords_removed'].apply(nltk.tag.pos_tag)
test_df['pos_tags'] = test_df['stopwords_removed'].apply(nltk.tag.pos_tag)

from nltk.corpus import wordnet


def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN
train_df['wordnet_pos'] = train_df['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])
test_df['wordnet_pos'] = test_df['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])

from nltk.stem import WordNetLemmatizer

wnl = WordNetLemmatizer()
train_df['lemmatized'] = train_df['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])
test_df['lemmatized'] = test_df['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])

preprocessing_train = train_df.loc[:, ['stopwords_removed', 'Sentiment']]
preprocessing_train

preprocessing_test = test_df.loc[:, ['stopwords_removed', 'Sentiment']]
preprocessing_test

preprocessing_train['tweets'] = [' '.join(map(str, l)) for l in preprocessing_train['stopwords_removed']]
preprocessing_train.head()

preprocessing_test['tweets'] = [' '.join(map(str, l)) for l in preprocessing_test['stopwords_removed']]
preprocessing_test.head()

x_train = preprocessing_train['tweets'].copy()
x_test = preprocessing_test['tweets'].copy()

y_train = preprocessing_train['Sentiment'].copy()
y_test = preprocessing_test['Sentiment'].copy()

max_len = np.max(x_train.apply(lambda x :len(x)))
max_len

from nltk.tokenize import word_tokenize,sent_tokenize
from bs4 import BeautifulSoup
from keras.preprocessing import text, sequence
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.model_selection import train_test_split
from string import punctuation
import keras
from keras.models import Sequential
import tensorflow as tf

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer()

tokenizer.fit_on_texts(x_train)
vocab_length = len(tokenizer.word_index) + 1

x_train = tokenizer.texts_to_sequences(x_train)
x_test = tokenizer.texts_to_sequences(x_test)

x_train = pad_sequences(x_train, maxlen=max_len, padding='post')
x_test = pad_sequences(x_test, maxlen=max_len, padding='post')

print("Vocab length:", vocab_length)
print("Max sequence length:", max_len)

embedding_dim = 16

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_length, embedding_dim, input_length=max_len),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(256, return_sequences=True)),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(3, activation='softmax')
])
# opt = tf.keras.optimizers.Adam(learning_rate=0.01)
model.compile(loss='categorical_crossentropy',optimizer="adam",metrics=['accuracy'])

print(model.summary())


from tensorflow.keras.utils import to_categorical
y_train = to_categorical(y_train, 3)
y_test = to_categorical(y_test, 3)

num_epochs = 5
history = model.fit(x_train, y_train, epochs=num_epochs, 
                    validation_data=(x_test, y_test))

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

print(f"Accuracy on training data is:- {acc[-1]*100} %")
print(f"Loss {loss[-1]*100}")

print(f"Accuracy on validation data is:- {val_acc[-1]*100} %")
print(f"Loss {val_loss[-1]*100}")

# save the model to disk
from keras.models import load_model
model.save('lstm_model.h5')
final_model = load_model('lstm_model.h5')

